{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "** *italicized text*BullRun.ai: Hybrid LSTM-Attention Model**  \n",
        "**Objective**: Predict 5-day forecasts for crypto prices (open, high, low, close) and trading volume using historical data.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Key Steps**  \n",
        "1. **Data Prep**:  \n",
        "   - **Input**: Daily prices, volume, marketCap for 5 cryptos (Bitcoin, Dogecoin, etc.).  \n",
        "   - **Processing**: Clean NaNs/zeros, normalize features (MinMaxScaler), create 10-day sliding windows.  \n",
        "\n",
        "\n",
        "2. **Model**:  \n",
        "   - **Bidirectional LSTM**: Captures past/future trends in price sequences.  \n",
        "   - **Attention Layer**: Focuses on critical time steps (e.g., volatility spikes).  \n",
        "   - **Output**: Predicts all 6 features (open, high, low, close, volume, marketCap).  \n",
        "\n",
        "\n",
        "3. **Training**:  \n",
        "   - **Time-Series CV**: Maintains temporal order in train-test splits.  \n",
        "   - **Regularization**: Dropout (30%) + L2 penalties to prevent overfitting.  \n",
        "   - **Callbacks**: Early stopping + adaptive learning rates.  \n",
        "\n",
        "\n",
        "4. **Forecasting**:  \n",
        "   - Autoregressive prediction: Iteratively feed outputs as inputs for 5-day forecasts.  \n",
        "   - Full precision preserved (critical for low-value coins like Dogecoin).  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Results**  \n",
        "- **Metrics**: MAE, MSE, R² (e.g., Bitcoin R² ~90-95%).  \n",
        "- **Output**: CSV with 5-day predictions for all cryptos, unrounded floats.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Why This Works**  \n",
        "- Handles multi-feature correlations (price + volume + marketCap).  \n",
        "- Balances sequential learning (LSTM) and feature weighting (Attention).  \n",
        "- Adaptable to volatile markets via iterative retraining.  \n",
        "\n",
        "\n",
        "**Use Case**: Short-term trading strategy planning for crypto investors.\n"
      ],
      "metadata": {
        "id": "ptlkpmkFFOh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BullRun.AI: Cryptocurrency Price Prediction Using Hybrid LSTM-Attention Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Objective**\n",
        "To develop a robust machine learning system that predicts **5-day future prices** (open, high, low, close) and trading volume for multiple cryptocurrencies, using historical market data. The model aims to capture complex temporal patterns in volatile markets while preserving precision for low-value assets like Dogecoin.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Motivation**\n",
        "Cryptocurrencies exhibit extreme volatility due to market sentiment, news, and macroeconomic factors. Accurate short-term predictions empower traders and investors to:\n",
        "- Optimize entry/exit points\n",
        "- Hedge risks\n",
        "- Automate trading strategies  \n",
        "Traditional statistical models (e.g., ARIMA) struggle with multi-feature crypto data. Deep learning architectures like **LSTMs** excel at sequence modeling, while **attention mechanisms** help focus on critical time steps. Combining these techniques improves forecast accuracy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Input Data**\n",
        "**Source**: Historical daily crypto data (CSV format) with columns:  \n",
        "- `date`, `crypto_name`, `open`, `high`, `low`, `close`, `volume`, `marketCap`  \n",
        "**Example Row**:  \n",
        "`2023-01-01, Bitcoin, 45000.25, 45500.30, 44500.15, 45200.50, 2500000000, 850000000000`\n",
        "\n",
        "\n",
        "**Key Characteristics**:\n",
        "- Time-series data (date-sorted)\n",
        "- Multiple cryptocurrencies in one dataset\n",
        "- High variance in values (e.g., Bitcoin vs. Dogecoin)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Data Preprocessing**\n",
        "1. **Filtering**:\n",
        "   - Select target cryptos (Bitcoin, Litecoin, XRP, Dogecoin, Monero)\n",
        "   - Remove rows with missing dates or invalid crypto names\n",
        "\n",
        "\n",
        "2. **Feature Selection**:\n",
        "   - Retain `open`, `high`, `low`, `close`, `volume`, `marketCap`  \n",
        "   *Rationale*: These capture price movement and market activity.\n",
        "\n",
        "\n",
        "3. **Cleaning**:\n",
        "   - Drop rows with `NaN` or zero volume (invalid trading days)\n",
        "   - Ensure chronological ordering by date\n",
        "\n",
        "\n",
        "4. **Normalization**:\n",
        "   - Apply `MinMaxScaler` to bound values between 0–1  \n",
        "   *Why?*: LSTMs require normalized data for stable training.  \n",
        "   *Formula*: \\( X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}} \\)\n",
        "\n",
        "\n",
        "5. **Windowing**:\n",
        "   - Convert time series into overlapping sequences using a **10-day window**  \n",
        "   *Example*: Days 1–10 → Predict Day 11; Days 2–11 → Predict Day 12  \n",
        "   *Output Shapes*:  \n",
        "     - Input (`X`): `(num_samples, 10, 6)`  \n",
        "     - Target (`y`): `(num_samples, 6)`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Model Architecture: Hybrid LSTM-Attention**\n",
        "A bidirectional LSTM processes sequences forward/backward, while attention identifies critical historical patterns.\n",
        "\n",
        "\n",
        "1. **Layers**:\n",
        "   - **Input**: 3D tensor `(batch_size, 10, 6)`  \n",
        "   - **Bidirectional LSTM**: 50 units per direction + dropout (30%) + L2 regularization  \n",
        "     *Purpose*: Learn long-term dependencies in both temporal directions.  \n",
        "   - **Attention**: Weights important timesteps dynamically  \n",
        "     *Mechanism*: \\( \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\)  \n",
        "   - **Concatenation**: Merge LSTM outputs with attention context  \n",
        "   - **Dense Layers**: 64 ReLU units → 6 linear outputs (one per feature)\n",
        "\n",
        "\n",
        "2. **Training**:\n",
        "   - **Loss**: Mean Squared Error (MSE)  \n",
        "   - **Optimizer**: Adam (learning rate = 0.001)  \n",
        "   - **Callbacks**:  \n",
        "     - Early stopping (patience=10)  \n",
        "     - Learning rate reduction on plateau  \n",
        "\n",
        "\n",
        "3. **Key Innovations**:\n",
        "   - **Multi-Feature Prediction**: Simultaneously forecasts all 6 features.  \n",
        "   - **Autoregressive Inference**: Uses predictions as input for subsequent days.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Training Process**\n",
        "1. **Time-Series Cross-Validation**:\n",
        "   - Split data into 5 folds using `TimeSeriesSplit`  \n",
        "   *Rationale*: Preserve temporal order during validation.\n",
        "\n",
        "\n",
        "2. **Batch Training**:\n",
        "   - Batch size: 32 (or smaller if data is limited)  \n",
        "   - Epochs: 50 (early stopping may terminate early)\n",
        "\n",
        "\n",
        "3. **Regularization**:\n",
        "   - **Dropout**: 30% neuron deactivation to prevent overfitting  \n",
        "   - **L2 Regularization**: Penalize large weights (\\( \\lambda = 0.001 \\))\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Prediction Workflow**\n",
        "1. **Last Window Extraction**:\n",
        "   - Use the most recent 10 days of scaled data  \n",
        "   *Shape*: `(1, 10, 6)`\n",
        "\n",
        "\n",
        "2. **Autoregressive Loop**:\n",
        "   For each of the next 5 days:  \n",
        "   - Predict all 6 features using the current window  \n",
        "   - Append prediction to the window  \n",
        "   - Remove oldest day to maintain 10-day window  \n",
        "\n",
        "\n",
        "3. **Inverse Scaling**:\n",
        "   - Convert normalized predictions back to original USD values  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Handling Low-Value Cryptos (e.g., Dogecoin)**\n",
        "- **Precision Preservation**:  \n",
        "  - Avoid rounding during CSV export (`float_format` disabled)  \n",
        "  - Example: Dogecoin’s predicted `open` = `0.000123456` (not rounded to 0.00)  \n",
        "- **Scaler Adaptation**:  \n",
        "  - `MinMaxScaler` handles small values without precision loss.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Validation & Verification**\n",
        "1. **Feature Alignment**:\n",
        "   - Ensure input/output features match (e.g., 6 in → 6 out)  \n",
        "   - Print actual vs predicted values during evaluation:  \n",
        "     ```\n",
        "     open: Predicted 45200.123 vs Actual 45150.456\n",
        "     marketCap: Predicted 8.5e11 vs Actual 8.6e11\n",
        "     ```\n",
        "\n",
        "\n",
        "2. **Metrics**:\n",
        "   - **MAE**: Average absolute error across all features  \n",
        "   - **MSE**: Emphasizes larger errors (sensitive to outliers)  \n",
        "   - **R²**: % variance explained (1.0 = perfect prediction)\n",
        "\n",
        "\n",
        "3. **Data Sufficiency Checks**:\n",
        "   - Require ≥11 days of data (10 for window + 1 prediction)  \n",
        "   - Skip cryptos with insufficient history.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Output**\n",
        "- **CSV File**: `cryptoprediction_full_precision.csv`  \n",
        "- **Columns**: `open, high, low, close, volume, marketCap, crypto_name, date`  \n",
        "- **Example Prediction**:  \n",
        "  ```\n",
        "  open      high       low      close    volume      marketCap   crypto_name   date\n",
        "  0.000123  0.000135  0.000118  0.000128  15234567   18273645    Dogecoin     2023-12-10\n",
        "  ```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Enhancements (Optional)**\n",
        "1. **Feature Engineering**: Add technical indicators (RSI, Bollinger Bands).  \n",
        "2. **Sentiment Integration**: Incorporate news/social media sentiment scores.  \n",
        "3. **Uncertainty Quantification**: Use Monte Carlo dropout for confidence intervals.  \n",
        "4. **Multi-Head Attention**: Capture diverse temporal patterns.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Conclusion**\n",
        "This approach provides a scalable framework for multi-crypto forecasting by:  \n",
        "- Leveraging bidirectional LSTMs for temporal dynamics  \n",
        "- Using attention to focus on critical historical patterns  \n",
        "- Ensuring precision for low-value assets  \n",
        "- Validating feature integrity at every stage  \n",
        "\n"
      ],
      "metadata": {
        "id": "GBC7OxqEdJKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you're in Colab or a new environment, uncomment to install packages:\n",
        "!pip install pandas numpy matplotlib scikit-learn tensorflow\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, Concatenate, Input, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "\n",
        "# If you have a GPU, this prints it out. (Optional in Colab)\n",
        "device = tf.config.list_physical_devices('GPU')\n",
        "if device:\n",
        "    print(f\"GPU found: {device}\")\n",
        "else:\n",
        "    print(\"No GPU found. Using CPU.\")\n",
        "\n",
        "# Mapping from crypto_name to shorter symbol used in plot filenames\n",
        "symbol_map = {\n",
        "    \"Bitcoin\": \"btc\",\n",
        "    \"Dogecoin\": \"doge\",\n",
        "    \"Ethereum\": \"eth\",\n",
        "    \"Litecoin\": \"ltc\"\n",
        "}\n",
        "\n",
        "def create_sliding_windows(df, window_len=5):\n",
        "    \"\"\"\n",
        "    Create overlapping windows of length `window_len` across df.\n",
        "    Each window is used as an input sequence for training.\n",
        "    \"\"\"\n",
        "    print(f\"[INFO] Creating sliding windows with window_len={window_len} ...\")\n",
        "    data = df.values\n",
        "    X = []\n",
        "    for i in range(len(df) - window_len):\n",
        "        X.append(data[i : i + window_len])\n",
        "    X = np.array(X)\n",
        "    print(f\"[INFO] Created X shape: {X.shape} (samples, window_len, features)\")\n",
        "    return X\n",
        "\n",
        "def build_hybrid_model(input_shape):\n",
        "    \"\"\"\n",
        "    Build a Stacked Bidirectional LSTM + Attention model predicting\n",
        "    the 4 features: [Price, High, Low, Close].\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Building a stacked Bidirectional LSTM + Attention model...\")\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # First Bidirectional LSTM\n",
        "    x = Bidirectional(\n",
        "        LSTM(\n",
        "            64,\n",
        "            return_sequences=True,\n",
        "            kernel_regularizer=l2(0.001)\n",
        "        )\n",
        "    )(inputs)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Second Bidirectional LSTM\n",
        "    x = Bidirectional(\n",
        "        LSTM(\n",
        "            64,\n",
        "            return_sequences=True,\n",
        "            kernel_regularizer=l2(0.001)\n",
        "        )\n",
        "    )(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention = Attention()([x, x])\n",
        "    attention = Dense(1, activation='tanh')(attention)\n",
        "\n",
        "    # Concatenate LSTM output with attention\n",
        "    concat = Concatenate()([x, attention])\n",
        "    concat = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(concat)\n",
        "\n",
        "    # Output layer (predicts all 4 features)\n",
        "    outputs = Dense(input_shape[-1], activation='linear')(concat[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    print(\"[INFO] Model built and compiled.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_predictions(actual, preds, prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Compute and print evaluation metrics.\n",
        "    prefix: Just a string to label each fold's results.\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(actual, preds)\n",
        "    mse = mean_squared_error(actual, preds)\n",
        "    r2 = r2_score(actual, preds)\n",
        "    print(f\"{prefix}MAE: {mae:.6f}\")\n",
        "    print(f\"{prefix}MSE: {mse:.6f}\")\n",
        "    print(f\"{prefix}R²: {r2:.6f}\")\n",
        "    return mae, mse, r2\n",
        "\n",
        "def predict_next_5_days(model, last_window_scaled, scaler=None, feature_cols=None):\n",
        "    \"\"\"\n",
        "    Predict the next 5 days (with full-precision)\n",
        "    given the last sliding window.\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Predicting the next 5 days...\")\n",
        "    predictions_scaled = []\n",
        "    current_window = last_window_scaled.copy()\n",
        "\n",
        "    for i in range(5):\n",
        "        next_day_scaled = model.predict(current_window, verbose=0)[0]\n",
        "        predictions_scaled.append(next_day_scaled)\n",
        "\n",
        "        # Shift the window and add new day at the end\n",
        "        new_window = np.roll(current_window, -1, axis=1)\n",
        "        new_window[0, -1, :] = next_day_scaled\n",
        "        current_window = new_window\n",
        "\n",
        "    predictions_scaled = np.array(predictions_scaled)\n",
        "    if scaler:\n",
        "        predictions = scaler.inverse_transform(predictions_scaled)\n",
        "    else:\n",
        "        predictions = predictions_scaled\n",
        "\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [f\"f{i}\" for i in range(predictions.shape[1])]\n",
        "\n",
        "    print(\"[INFO] Predictions for 5 future days generated.\")\n",
        "    return pd.DataFrame(predictions, columns=feature_cols)\n",
        "\n",
        "def plot_4_subplots_for_7_and_5_days(\n",
        "    last_7: pd.DataFrame,\n",
        "    predicted_5: pd.DataFrame,\n",
        "    crypto: str,\n",
        "    out_dir: str = \"\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates one figure with 4 subplots (Price, High, Low, Close).\n",
        "    - Last 7 days in a solid line\n",
        "    - Next 5 days in a dashed line\n",
        "    Saves as <symbol>_prediction.png (e.g., btc_prediction.png).\n",
        "    \"\"\"\n",
        "    features = [\"Price\", \"High\", \"Low\", \"Close\"]\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True)\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        ax = axes.flat[i]\n",
        "\n",
        "        # Plot last 7 days (actual) in solid line\n",
        "        ax.plot(\n",
        "            last_7[\"Date\"],\n",
        "            last_7[feature],\n",
        "            label=\"Actual\",\n",
        "            marker='o',\n",
        "            color='blue'\n",
        "        )\n",
        "\n",
        "        # Plot next 5 days (predicted) in dashed line\n",
        "        ax.plot(\n",
        "            predicted_5[\"Date\"],\n",
        "            predicted_5[feature],\n",
        "            label=\"Predicted\",\n",
        "            linestyle='--',\n",
        "            marker='x',\n",
        "            color='red'\n",
        "        )\n",
        "\n",
        "        ax.set_title(feature)\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "        # Rotate x-ticks\n",
        "        for tick in ax.get_xticklabels():\n",
        "            tick.set_rotation(45)\n",
        "\n",
        "    fig.suptitle(f\"{crypto} - Last 7 days + Next 5 days Prediction\")\n",
        "    plt.tight_layout()\n",
        "    # Adjust top to ensure suptitle isn't clipped\n",
        "    plt.subplots_adjust(top=0.90)\n",
        "\n",
        "    symbol = symbol_map.get(crypto, crypto.lower())\n",
        "    out_file = f\"{out_dir}{symbol}_prediction.png\"\n",
        "    plt.savefig(out_file)\n",
        "    print(f\"[INFO] Plot saved as: {out_file}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ====================== MAIN SCRIPT ======================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[INFO] Reading dataset...\")\n",
        "    csv_path = \"btc_doge_eth_ltc-alldata.csv\"  # Make sure it has the correct columns\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"[INFO] Dataset loaded. Shape: {df.shape}\")\n",
        "\n",
        "    # Date handling: parse in DD-MM-YYYY format (dayfirst=True)\n",
        "    print(\"[INFO] Converting 'Date' column to datetime (DD-MM-YYYY)...\")\n",
        "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
        "    df = df.dropna(subset=['Date'])  # drop rows that fail conversion\n",
        "    print(f\"[INFO] Dataset shape after dropping invalid dates: {df.shape}\")\n",
        "\n",
        "    # Ensure consistent crypto naming (title-case)\n",
        "    df['crypto_name'] = df['crypto_name'].str.title()\n",
        "\n",
        "    # List of cryptos present in your dataset\n",
        "    cryptos = [\"Bitcoin\", \"Dogecoin\", \"Ethereum\", \"Litecoin\"]\n",
        "    print(f\"[INFO] Cryptos to process: {cryptos}\")\n",
        "\n",
        "    # DataFrame to store all future-day predictions\n",
        "    all_predictions = pd.DataFrame()\n",
        "\n",
        "    # For each crypto...\n",
        "    for crypto in cryptos:\n",
        "        print(f\"\\n{'='*40}\\n[INFO] PROCESSING {crypto.upper()}\\n{'='*40}\")\n",
        "\n",
        "        # Subset the data for the specific crypto\n",
        "        sub_df = df[df['crypto_name'] == crypto].copy()\n",
        "        sub_df.set_index('Date', inplace=True)\n",
        "        sub_df.sort_index(inplace=True)\n",
        "\n",
        "        # Keep only [Price, High, Low, Close]\n",
        "        required_features = ['Price', 'High', 'Low', 'Close']\n",
        "        print(\"[INFO] Selecting required features:\", required_features)\n",
        "        sub_df = sub_df[required_features].dropna()\n",
        "        print(f\"[INFO] After dropping NaNs, data shape for {crypto}: {sub_df.shape}\")\n",
        "\n",
        "        # We need enough data for a 10-day window + at least 1 test sample\n",
        "        if len(sub_df) < 11:\n",
        "            print(f\"[WARNING] Insufficient data for {crypto}. \"\n",
        "                  f\"Needs at least 11 rows, found {len(sub_df)}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Scale features\n",
        "        print(\"[INFO] Applying MinMaxScaler on 4 features.\")\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_data = scaler.fit_transform(sub_df)\n",
        "        sub_df_scaled = pd.DataFrame(scaled_data, columns=sub_df.columns, index=sub_df.index)\n",
        "        print(\"[INFO] Scaling complete. Example of scaled values:\")\n",
        "        print(sub_df_scaled.head())\n",
        "\n",
        "        # Create windows of length 10\n",
        "        window_len = 10\n",
        "        X = create_sliding_windows(sub_df_scaled, window_len)\n",
        "        # Targets start after the first window_len\n",
        "        y = sub_df_scaled.iloc[window_len:].values  # shape: (#samples, 4)\n",
        "        print(f\"[INFO] y shape: {y.shape} (samples, features)\")\n",
        "\n",
        "        # TimeSeriesSplit for train/test\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        print(f\"[INFO] Performing 5-fold TimeSeriesSplit cross-validation on {crypto}...\")\n",
        "\n",
        "        # For collecting cross-validation results\n",
        "        fold_metrics = []\n",
        "\n",
        "        # Cross-validation loop\n",
        "        for fold_idx, (train_index, test_index) in enumerate(tscv.split(X), start=1):\n",
        "            print(f\"\\n[INFO] ---- Fold {fold_idx} of 5 ----\")\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            print(f\"[INFO] X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "            print(f\"[INFO] X_test  shape: {X_test.shape},  y_test  shape: {y_test.shape}\")\n",
        "\n",
        "            # Build a fresh model each fold\n",
        "            model = build_hybrid_model((window_len, len(required_features)))\n",
        "\n",
        "            # Train the model with verbose=1 to see the epoch logs\n",
        "            print(\"[INFO] Training model...\")\n",
        "            history = model.fit(\n",
        "                X_train,\n",
        "                y_train,\n",
        "                validation_data=(X_test, y_test),\n",
        "                epochs=60,  # a bit more than before\n",
        "                batch_size=32,\n",
        "                callbacks=[\n",
        "                    EarlyStopping(patience=10, restore_best_weights=True),\n",
        "                    ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "                ],\n",
        "                verbose=1  # prints full training logs\n",
        "            )\n",
        "\n",
        "            print(\"[INFO] Evaluating fold performance...\")\n",
        "            preds_scaled = model.predict(X_test, verbose=0)\n",
        "            preds = scaler.inverse_transform(preds_scaled)\n",
        "            actual = scaler.inverse_transform(y_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            print(\"[INFO] Fold metrics:\")\n",
        "            mae, mse, r2 = evaluate_predictions(actual, preds, prefix=\"  \")\n",
        "            fold_metrics.append((mae, mse, r2))\n",
        "\n",
        "        # Average metrics over all folds\n",
        "        if fold_metrics:\n",
        "            avg_mae = np.mean([m[0] for m in fold_metrics])\n",
        "            avg_mse = np.mean([m[1] for m in fold_metrics])\n",
        "            avg_r2 = np.mean([m[2] for m in fold_metrics])\n",
        "            print(f\"\\n[INFO] Average Cross-Validation Metrics for {crypto}:\")\n",
        "            print(f\"  MAE = {avg_mae:.6f}\")\n",
        "            print(f\"  MSE = {avg_mse:.6f}\")\n",
        "            print(f\"  R²  = {avg_r2:.6f}\")\n",
        "        else:\n",
        "            print(f\"[WARNING] No metrics computed for {crypto}. Possibly no valid folds.\")\n",
        "            continue\n",
        "\n",
        "        print(\"\\n[INFO] ----------- FINAL RETRAIN on ALL data -----------\")\n",
        "        # After cross-validation, we build a final model\n",
        "        # using all samples for training to predict the future.\n",
        "        model_final = build_hybrid_model((window_len, len(required_features)))\n",
        "        print(\"[INFO] Training final model on the entire dataset (X, y) ...\")\n",
        "        model_final.fit(\n",
        "            X, y,\n",
        "            epochs=60,\n",
        "            batch_size=32,\n",
        "            callbacks=[\n",
        "                EarlyStopping(patience=10, restore_best_weights=True),\n",
        "                ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "            ],\n",
        "            verbose=1  # prints training logs\n",
        "        )\n",
        "\n",
        "        # Predict next 5 days\n",
        "        print(\"[INFO] Predicting next 5 days based on last window of scaled data...\")\n",
        "        last_window = sub_df_scaled.tail(window_len).values.reshape(1, window_len, -1)\n",
        "        predictions = predict_next_5_days(model_final, last_window, scaler, required_features)\n",
        "\n",
        "        # Create a date range for the next 5 predictions\n",
        "        predictions['Date'] = pd.date_range(\n",
        "            start=sub_df.index[-1] + pd.Timedelta(days=1),\n",
        "            periods=5\n",
        "        )\n",
        "        predictions['crypto_name'] = crypto\n",
        "\n",
        "        # Reorder columns\n",
        "        predictions = predictions[['Price', 'High', 'Low', 'Close', 'crypto_name', 'Date']]\n",
        "        all_predictions = pd.concat([all_predictions, predictions], ignore_index=True)\n",
        "        print(\"[INFO] 5-day future predictions for\", crypto, \"complete.\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Plot last 7 + next 5 days in 4 subplots\n",
        "        # -----------------------------\n",
        "        print(\"[INFO] Creating a 4-subplot chart for last 7 days + next 5 days ...\")\n",
        "        # Last 7 days (unscaled) from sub_df\n",
        "        last_7_df = sub_df.tail(7).reset_index()  # keep the 'Date' as a column\n",
        "\n",
        "        # The \"predictions\" DF is already unscaled\n",
        "        predicted_5_df = predictions[['Date','Price','High','Low','Close']].copy()\n",
        "\n",
        "        # Create the 4-subplot figure\n",
        "        plot_4_subplots_for_7_and_5_days(\n",
        "            last_7_df,\n",
        "            predicted_5_df,\n",
        "            crypto=crypto\n",
        "        )\n",
        "\n",
        "    # Save all predictions to a single CSV\n",
        "    if not all_predictions.empty:\n",
        "        out_file = \"btc_doge_eth_ltc_predictions.csv\"\n",
        "        print(f\"\\n[INFO] Saving all predictions to {out_file} ...\")\n",
        "        all_predictions.to_csv(out_file, index=False)\n",
        "        print(\"[INFO] Predictions saved successfully. Preview:\")\n",
        "        print(\n",
        "            all_predictions.head(10).to_string(\n",
        "                float_format=lambda x: f\"{x:.10f}\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"[WARNING] No predictions generated for any crypto.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyIJgiCAT3_N",
        "outputId": "b1b1827b-e7ad-4c0c-85fd-52c42b3f182c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "GPU found: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[INFO] Reading dataset...\n",
            "[INFO] Dataset loaded. Shape: (12950, 6)\n",
            "[INFO] Converting 'Date' column to datetime (DD-MM-YYYY)...\n",
            "[INFO] Dataset shape after dropping invalid dates: (12950, 6)\n",
            "[INFO] Cryptos to process: ['Bitcoin', 'Dogecoin', 'Ethereum', 'Litecoin']\n",
            "\n",
            "========================================\n",
            "[INFO] PROCESSING BITCOIN\n",
            "========================================\n",
            "[INFO] Selecting required features: ['Price', 'High', 'Low', 'Close']\n",
            "[INFO] After dropping NaNs, data shape for Bitcoin: (3812, 4)\n",
            "[INFO] Applying MinMaxScaler on 4 features.\n",
            "[INFO] Scaling complete. Example of scaled values:\n",
            "               Price      High       Low     Close\n",
            "Date                                              \n",
            "2014-09-17  0.002635  0.002672  0.002727  0.002355\n",
            "2014-09-18  0.002325  0.002298  0.002642  0.002251\n",
            "2014-09-19  0.002045  0.002026  0.002333  0.001984\n",
            "2014-09-20  0.002178  0.002077  0.002055  0.001943\n",
            "2014-09-21  0.002083  0.002109  0.002182  0.001843\n",
            "[INFO] Creating sliding windows with window_len=10 ...\n",
            "[INFO] Created X shape: (3802, 10, 4) (samples, window_len, features)\n",
            "[INFO] y shape: (3802, 4) (samples, features)\n",
            "[INFO] Performing 5-fold TimeSeriesSplit cross-validation on Bitcoin...\n",
            "\n",
            "[INFO] ---- Fold 1 of 5 ----\n",
            "[INFO] X_train shape: (637, 10, 4), y_train shape: (637, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 44ms/step - loss: 0.3976 - val_loss: 0.2786 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2449 - val_loss: 0.1676 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1455 - val_loss: 0.0988 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0843 - val_loss: 0.0575 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0479 - val_loss: 0.0334 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0267 - val_loss: 0.0197 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0147 - val_loss: 0.0120 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0080 - val_loss: 0.0078 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0055 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0042 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0036 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.7043e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.5882e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.9062e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0027e-04 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.2491e-05 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.7169e-05 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.3912e-05 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.2133e-06 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 4.1191e-06 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 2.3179e-06 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.6946e-06 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.1511e-06 - val_loss: 0.0028 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2630e-06 - val_loss: 0.0028 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1141e-06 - val_loss: 0.0028 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.8052e-07 - val_loss: 0.0028 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.6795e-07 - val_loss: 0.0028 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.0300e-06 - val_loss: 0.0028 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.6787e-07 - val_loss: 0.0028 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 8.7185e-07 - val_loss: 0.0028 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.6379e-07 - val_loss: 0.0028 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.9820e-07 - val_loss: 0.0028 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.5712e-07 - val_loss: 0.0028 - learning_rate: 1.2500e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.9346e-07 - val_loss: 0.0028 - learning_rate: 1.2500e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.6813e-07 - val_loss: 0.0028 - learning_rate: 1.2500e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.7329e-07 - val_loss: 0.0028 - learning_rate: 1.2500e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 3499.997608\n",
            "  MSE: 31541294.957611\n",
            "  R²: -0.636703\n",
            "\n",
            "[INFO] ---- Fold 2 of 5 ----\n",
            "[INFO] X_train shape: (1270, 10, 4), y_train shape: (1270, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.3575 - val_loss: 0.1645 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1293 - val_loss: 0.0548 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0424 - val_loss: 0.0169 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0130 - val_loss: 0.0050 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 7.9867e-04 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.9637e-04 - val_loss: 2.2364e-04 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2.5970e-04 - val_loss: 6.6581e-04 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.2738e-04 - val_loss: 7.9377e-05 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.1460e-05 - val_loss: 7.6177e-05 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.9322e-05 - val_loss: 1.1245e-04 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1.5891e-04 - val_loss: 2.7027e-04 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.0668e-04 - val_loss: 6.0431e-05 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.9123e-05 - val_loss: 6.2673e-05 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 6.5777e-05 - val_loss: 4.6198e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.5497e-05 - val_loss: 4.6191e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.2314e-05 - val_loss: 3.8099e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.2862e-05 - val_loss: 3.7076e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.6573e-05 - val_loss: 3.5433e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.3076e-05 - val_loss: 6.3400e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.6536e-05 - val_loss: 5.0172e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5.1894e-05 - val_loss: 4.3793e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.5389e-05 - val_loss: 7.4195e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.6169e-05 - val_loss: 3.7889e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.1064e-05 - val_loss: 3.8771e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5.0029e-05 - val_loss: 4.3877e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.1984e-05 - val_loss: 3.5409e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.4233e-05 - val_loss: 3.2796e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.0409e-05 - val_loss: 5.2016e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.6194e-05 - val_loss: 3.1077e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 31/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.0184e-05 - val_loss: 3.1287e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 32/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.9209e-05 - val_loss: 6.3894e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 33/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.6376e-05 - val_loss: 5.4373e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 34/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.9469e-05 - val_loss: 3.2238e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 35/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3.5854e-05 - val_loss: 3.9931e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 36/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 3.8316e-05 - val_loss: 3.3926e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 37/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.1498e-05 - val_loss: 3.6244e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 38/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.5210e-05 - val_loss: 3.0466e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 39/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.7352e-05 - val_loss: 5.8086e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 40/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.9216e-05 - val_loss: 3.1938e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 41/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.5767e-05 - val_loss: 3.1104e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 42/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.5877e-05 - val_loss: 3.7589e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.8361e-05 - val_loss: 3.1809e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.8162e-05 - val_loss: 3.8554e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.6514e-05 - val_loss: 3.1763e-05 - learning_rate: 7.8125e-06\n",
            "Epoch 46/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3.5361e-05 - val_loss: 3.5111e-05 - learning_rate: 7.8125e-06\n",
            "Epoch 47/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3.9238e-05 - val_loss: 3.0335e-05 - learning_rate: 7.8125e-06\n",
            "Epoch 48/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.7099e-05 - val_loss: 3.4166e-05 - learning_rate: 7.8125e-06\n",
            "Epoch 49/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.0286e-05 - val_loss: 3.3345e-05 - learning_rate: 7.8125e-06\n",
            "Epoch 50/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.4499e-05 - val_loss: 3.1250e-05 - learning_rate: 3.9063e-06\n",
            "Epoch 51/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.5179e-05 - val_loss: 3.0689e-05 - learning_rate: 3.9063e-06\n",
            "Epoch 52/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.5148e-05 - val_loss: 3.1262e-05 - learning_rate: 3.9063e-06\n",
            "Epoch 53/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.7864e-05 - val_loss: 3.1345e-05 - learning_rate: 3.9063e-06\n",
            "Epoch 54/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.5680e-05 - val_loss: 3.3087e-05 - learning_rate: 3.9063e-06\n",
            "Epoch 55/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.7724e-05 - val_loss: 3.0871e-05 - learning_rate: 1.9531e-06\n",
            "Epoch 56/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.2035e-05 - val_loss: 3.1753e-05 - learning_rate: 1.9531e-06\n",
            "Epoch 57/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.7686e-05 - val_loss: 3.1431e-05 - learning_rate: 1.9531e-06\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 280.751275\n",
            "  MSE: 137454.204435\n",
            "  R²: 0.973168\n",
            "\n",
            "[INFO] ---- Fold 3 of 5 ----\n",
            "[INFO] X_train shape: (1903, 10, 4), y_train shape: (1903, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.3269 - val_loss: 0.0999 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0676 - val_loss: 0.0228 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0116 - val_loss: 0.0148 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0231 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.7416e-04 - val_loss: 0.0176 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.8733e-04 - val_loss: 0.0198 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.7546e-05 - val_loss: 0.0216 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.1875e-04 - val_loss: 0.0184 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.6968e-05 - val_loss: 0.0224 - learning_rate: 5.0000e-04\n",
            "Epoch 10/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.1837e-05 - val_loss: 0.0229 - learning_rate: 5.0000e-04\n",
            "Epoch 11/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.3221e-05 - val_loss: 0.0200 - learning_rate: 5.0000e-04\n",
            "Epoch 12/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 5.1275e-05 - val_loss: 0.0198 - learning_rate: 5.0000e-04\n",
            "Epoch 13/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5.1725e-05 - val_loss: 0.0203 - learning_rate: 5.0000e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 6893.035409\n",
            "  MSE: 136298819.484386\n",
            "  R²: 0.564705\n",
            "\n",
            "[INFO] ---- Fold 4 of 5 ----\n",
            "[INFO] X_train shape: (2536, 10, 4), y_train shape: (2536, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3092 - val_loss: 0.0607 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0393 - val_loss: 0.0079 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0055 - val_loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0011 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 6.4121e-04 - val_loss: 0.0054 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 8.6647e-04 - val_loss: 7.2579e-04 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 6.7700e-04 - val_loss: 9.0675e-04 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.4212e-04 - val_loss: 9.6805e-04 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 4.6744e-04 - val_loss: 6.2455e-04 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.6085e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.4437e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.9434e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.2254e-04 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.7164e-04 - val_loss: 0.0027 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 5.6781e-04 - val_loss: 7.0939e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.8707e-04 - val_loss: 6.0425e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.3691e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.5315e-04 - val_loss: 5.2207e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.3190e-04 - val_loss: 7.3191e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 4.6923e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 4.3347e-04 - val_loss: 4.6946e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.0059e-04 - val_loss: 6.6686e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.5745e-04 - val_loss: 4.8111e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.7838e-04 - val_loss: 0.0016 - learning_rate: 2.5000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 3.4198e-04 - val_loss: 4.6938e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.1375e-04 - val_loss: 4.9905e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.9307e-04 - val_loss: 4.5854e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.4507e-04 - val_loss: 5.0537e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.6367e-04 - val_loss: 6.4035e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.6807e-04 - val_loss: 4.3038e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.5482e-04 - val_loss: 4.8411e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.2661e-04 - val_loss: 4.1604e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.4237e-04 - val_loss: 7.5401e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0631e-04 - val_loss: 5.7662e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2.3676e-04 - val_loss: 8.0265e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 2.4930e-04 - val_loss: 5.1240e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.5393e-04 - val_loss: 4.0610e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.3401e-04 - val_loss: 3.8166e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 40/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.4337e-04 - val_loss: 4.0955e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 41/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.2604e-04 - val_loss: 5.8840e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 42/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.3932e-04 - val_loss: 3.8260e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.0450e-04 - val_loss: 4.2372e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.3938e-04 - val_loss: 3.7986e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.4272e-04 - val_loss: 4.1759e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.0350e-04 - val_loss: 4.9936e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2.2387e-04 - val_loss: 3.9005e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.1823e-04 - val_loss: 4.0139e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.2214e-04 - val_loss: 3.8592e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.1453e-04 - val_loss: 3.8689e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.8991e-04 - val_loss: 4.6060e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.0166e-04 - val_loss: 3.9886e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.1057e-04 - val_loss: 3.7459e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.9333e-04 - val_loss: 3.9592e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 55/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.9059e-04 - val_loss: 3.9083e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 56/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.3218e-04 - val_loss: 4.1626e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 57/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.1023e-04 - val_loss: 3.8476e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 58/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 2.0705e-04 - val_loss: 3.8727e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 59/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.0225e-04 - val_loss: 3.9016e-04 - learning_rate: 3.9063e-06\n",
            "Epoch 60/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.0855e-04 - val_loss: 3.9683e-04 - learning_rate: 3.9063e-06\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 1240.584912\n",
            "  MSE: 3346603.887187\n",
            "  R²: 0.981201\n",
            "\n",
            "[INFO] ---- Fold 5 of 5 ----\n",
            "[INFO] X_train shape: (3169, 10, 4), y_train shape: (3169, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.2912 - val_loss: 0.0483 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0246 - val_loss: 0.0075 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0118 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0068 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0013 - val_loss: 0.0122 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.6394e-04 - val_loss: 0.0157 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.7707e-04 - val_loss: 0.0154 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0025 - val_loss: 0.0103 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.9883e-04 - val_loss: 0.0301 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0011 - val_loss: 0.0128 - learning_rate: 5.0000e-04\n",
            "Epoch 11/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5.0368e-04 - val_loss: 0.0119 - learning_rate: 5.0000e-04\n",
            "Epoch 12/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 4.7142e-04 - val_loss: 0.0128 - learning_rate: 5.0000e-04\n",
            "Epoch 13/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 4.3314e-04 - val_loss: 0.0127 - learning_rate: 5.0000e-04\n",
            "Epoch 14/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 4.7450e-04 - val_loss: 0.0169 - learning_rate: 5.0000e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 4692.872315\n",
            "  MSE: 69112248.571107\n",
            "  R²: 0.873591\n",
            "\n",
            "[INFO] Average Cross-Validation Metrics for Bitcoin:\n",
            "  MAE = 3321.448304\n",
            "  MSE = 48087284.220945\n",
            "  R²  = 0.551192\n",
            "\n",
            "[INFO] ----------- FINAL RETRAIN on ALL data -----------\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training final model on the entire dataset (X, y) ...\n",
            "Epoch 1/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.2866 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m  9/119\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0341"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0235 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0060 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0033 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.6193e-04 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 8.7922e-04 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.8848e-04 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.7923e-04 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.0769e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.2509e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.9288e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.6838e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.2538e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 9.3765e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.2506e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.9783e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.0439e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.5284e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.4012e-04 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.0284e-04 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.2036e-04 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 6.7838e-04 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 6.3338e-04 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.7083e-04 - learning_rate: 0.0010\n",
            "Epoch 32/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.1278e-04 - learning_rate: 0.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.5409e-04 - learning_rate: 0.0010\n",
            "Epoch 34/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.0211e-04 - learning_rate: 0.0010\n",
            "Epoch 35/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.8373e-04 - learning_rate: 0.0010\n",
            "Epoch 36/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.1517e-04 - learning_rate: 0.0010\n",
            "Epoch 37/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 8.4667e-04 - learning_rate: 0.0010\n",
            "Epoch 38/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 6.4219e-04 - learning_rate: 0.0010\n",
            "Epoch 39/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.5652e-04 - learning_rate: 0.0010\n",
            "Epoch 40/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.2550e-04 - learning_rate: 0.0010\n",
            "Epoch 41/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.5605e-04 - learning_rate: 0.0010\n",
            "Epoch 42/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.1166e-04 - learning_rate: 0.0010\n",
            "Epoch 43/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.4000e-04 - learning_rate: 0.0010\n",
            "Epoch 44/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.0087e-04 - learning_rate: 0.0010\n",
            "Epoch 45/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.8227e-04 - learning_rate: 0.0010\n",
            "Epoch 46/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 7.9122e-04 - learning_rate: 0.0010\n",
            "Epoch 47/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.7358e-04 - learning_rate: 0.0010\n",
            "Epoch 48/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 6.4043e-04 - learning_rate: 0.0010\n",
            "Epoch 49/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.3993e-04 - learning_rate: 0.0010\n",
            "Epoch 50/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.4090e-04 - learning_rate: 0.0010\n",
            "Epoch 51/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.9220e-04 - learning_rate: 0.0010\n",
            "Epoch 52/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.9962e-04 - learning_rate: 0.0010\n",
            "Epoch 53/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.9410e-04 - learning_rate: 0.0010\n",
            "Epoch 54/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 5.4351e-04 - learning_rate: 0.0010\n",
            "Epoch 55/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 5.0747e-04 - learning_rate: 0.0010\n",
            "Epoch 56/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.1300e-04 - learning_rate: 0.0010\n",
            "Epoch 57/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.0238e-04 - learning_rate: 0.0010\n",
            "Epoch 58/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.7632e-04 - learning_rate: 0.0010\n",
            "Epoch 59/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.4235e-04 - learning_rate: 0.0010\n",
            "Epoch 60/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.1792e-04 - learning_rate: 0.0010\n",
            "[INFO] Predicting next 5 days based on last window of scaled data...\n",
            "[INFO] Predicting the next 5 days...\n",
            "[INFO] Predictions for 5 future days generated.\n",
            "[INFO] 5-day future predictions for Bitcoin complete.\n",
            "[INFO] Creating a 4-subplot chart for last 7 days + next 5 days ...\n",
            "[INFO] Plot saved as: btc_prediction.png\n",
            "\n",
            "========================================\n",
            "[INFO] PROCESSING DOGECOIN\n",
            "========================================\n",
            "[INFO] Selecting required features: ['Price', 'High', 'Low', 'Close']\n",
            "[INFO] After dropping NaNs, data shape for Dogecoin: (2663, 4)\n",
            "[INFO] Applying MinMaxScaler on 4 features.\n",
            "[INFO] Scaling complete. Example of scaled values:\n",
            "               Price      High       Low     Close\n",
            "Date                                              \n",
            "2017-11-09  0.000551  0.000295  0.000234  0.000278\n",
            "2017-11-10  0.000183  0.000203  0.000546  0.000300\n",
            "2017-11-11  0.000238  0.000229  0.000146  0.000064\n",
            "2017-11-12  0.000000  0.000000  0.000208  0.000000\n",
            "2017-11-13  0.000253  0.000028  0.000000  0.000003\n",
            "[INFO] Creating sliding windows with window_len=10 ...\n",
            "[INFO] Created X shape: (2653, 10, 4) (samples, window_len, features)\n",
            "[INFO] y shape: (2653, 4) (samples, features)\n",
            "[INFO] Performing 5-fold TimeSeriesSplit cross-validation on Dogecoin...\n",
            "\n",
            "[INFO] ---- Fold 1 of 5 ----\n",
            "[INFO] X_train shape: (443, 10, 4), y_train shape: (443, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - loss: 0.4065 - val_loss: 0.3162 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2913 - val_loss: 0.2227 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2043 - val_loss: 0.1544 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1413 - val_loss: 0.1058 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0966 - val_loss: 0.0717 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0653 - val_loss: 0.0481 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0438 - val_loss: 0.0320 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0291 - val_loss: 0.0212 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0192 - val_loss: 0.0139 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0126 - val_loss: 0.0090 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0082 - val_loss: 0.0059 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0053 - val_loss: 0.0038 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.3862e-04 - val_loss: 6.6734e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.0661e-04 - val_loss: 4.3012e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 3.9307e-04 - val_loss: 2.7913e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.5612e-04 - val_loss: 1.8214e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.7090e-04 - val_loss: 1.1982e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.1073e-04 - val_loss: 7.9887e-05 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.3408e-05 - val_loss: 5.1496e-05 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.9795e-05 - val_loss: 3.1387e-05 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 3.6523e-05 - val_loss: 2.3061e-05 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.7492e-05 - val_loss: 1.2565e-05 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.1840e-05 - val_loss: 1.2987e-05 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.8678e-05 - val_loss: 1.1064e-05 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.3210e-05 - val_loss: 1.1857e-05 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.3522e-05 - val_loss: 5.6762e-06 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1810e-05 - val_loss: 7.6924e-06 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.1975e-05 - val_loss: 9.8186e-06 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.1402e-05 - val_loss: 5.6519e-06 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1633e-05 - val_loss: 7.9318e-06 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0945e-05 - val_loss: 5.5909e-06 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.2212e-05 - val_loss: 4.8274e-06 - learning_rate: 5.0000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.1220e-05 - val_loss: 7.0170e-06 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3000e-05 - val_loss: 5.5216e-06 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1406e-05 - val_loss: 6.7843e-06 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.2912e-05 - val_loss: 5.8631e-06 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.1840e-05 - val_loss: 5.6715e-06 - learning_rate: 2.5000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.3446e-05 - val_loss: 6.5618e-06 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3050e-05 - val_loss: 6.6249e-06 - learning_rate: 1.2500e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.0704e-05 - val_loss: 6.0140e-06 - learning_rate: 1.2500e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.1900e-05 - val_loss: 6.2006e-06 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.0909e-05 - val_loss: 6.8703e-06 - learning_rate: 1.2500e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 0.001376\n",
            "  MSE: 0.000002\n",
            "  R²: -9.185185\n",
            "\n",
            "[INFO] ---- Fold 2 of 5 ----\n",
            "[INFO] X_train shape: (885, 10, 4), y_train shape: (885, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 0.3810 - val_loss: 0.2768 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1905 - val_loss: 0.1606 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0897 - val_loss: 0.1031 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0406 - val_loss: 0.0759 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0177 - val_loss: 0.0636 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0076 - val_loss: 0.0583 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0561 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0551 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.5771e-04 - val_loss: 0.0545 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3304e-04 - val_loss: 0.0542 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.9720e-05 - val_loss: 0.0542 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3994e-05 - val_loss: 0.0544 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.0544e-05 - val_loss: 0.0543 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.1804e-05 - val_loss: 0.0544 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 8.2775e-06 - val_loss: 0.0543 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.8851e-06 - val_loss: 0.0544 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 7.6332e-06 - val_loss: 0.0543 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 7.9494e-06 - val_loss: 0.0545 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 7.3659e-06 - val_loss: 0.0542 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 9.1869e-06 - val_loss: 0.0544 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.4543e-06 - val_loss: 0.0544 - learning_rate: 2.5000e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 0.074990\n",
            "  MSE: 0.025096\n",
            "  R²: -0.279252\n",
            "\n",
            "[INFO] ---- Fold 3 of 5 ----\n",
            "[INFO] X_train shape: (1327, 10, 4), y_train shape: (1327, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 0.3685 - val_loss: 0.1674 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1324 - val_loss: 0.0585 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0462 - val_loss: 0.0208 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0175 - val_loss: 0.0116 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0080 - val_loss: 0.0048 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0032 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0043 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0079 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0145 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 8.6908e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0077 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 9.9668e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 0.0043 - learning_rate: 5.0000e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 0.010807\n",
            "  MSE: 0.000201\n",
            "  R²: 0.962421\n",
            "\n",
            "[INFO] ---- Fold 4 of 5 ----\n",
            "[INFO] X_train shape: (1769, 10, 4), y_train shape: (1769, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.3439 - val_loss: 0.1187 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0882 - val_loss: 0.0289 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0227 - val_loss: 0.0081 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0073 - val_loss: 0.0034 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 7.3414e-04 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 9.9768e-04 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 6.3767e-04 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 6.3883e-04 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 7.8060e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 5.0183e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 9.7377e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 7.6569e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 5.8540e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 4.6860e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 6.6372e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0035 - val_loss: 4.9030e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.0133e-04 - val_loss: 8.2061e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 7.8461e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 3.6775e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 9.5482e-04 - val_loss: 4.3545e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 4.9388e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.2059e-04 - val_loss: 3.5516e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 3.4265e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 3.4403e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.2212e-04 - val_loss: 3.7661e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 8.3136e-04 - val_loss: 3.7841e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.5477e-04 - val_loss: 3.4206e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.8557e-04 - val_loss: 4.0983e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 4.3245e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.7335e-04 - val_loss: 3.7017e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.0015e-04 - val_loss: 4.0182e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 4.8837e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.1613e-04 - val_loss: 3.8767e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 8.6836e-04 - val_loss: 3.3455e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.4140e-04 - val_loss: 3.6112e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 41/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 3.5685e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 42/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.0248e-04 - val_loss: 3.9073e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.0646e-04 - val_loss: 2.8786e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.8887e-04 - val_loss: 2.8093e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 8.3905e-04 - val_loss: 3.2515e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.8146e-04 - val_loss: 2.9275e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.1799e-04 - val_loss: 3.3411e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.9744e-04 - val_loss: 3.4662e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.3715e-04 - val_loss: 3.2000e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 8.0961e-04 - val_loss: 3.0938e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 8.1499e-04 - val_loss: 3.2243e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.3289e-04 - val_loss: 3.1268e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.2116e-04 - val_loss: 3.0461e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.3595e-04 - val_loss: 3.3239e-04 - learning_rate: 1.5625e-05\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 0.002995\n",
            "  MSE: 0.000028\n",
            "  R²: 0.841312\n",
            "\n",
            "[INFO] ---- Fold 5 of 5 ----\n",
            "[INFO] X_train shape: (2211, 10, 4), y_train shape: (2211, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 0.3247 - val_loss: 0.0830 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0569 - val_loss: 0.0183 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0116 - val_loss: 0.0047 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0029 - val_loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0026 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0040 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0101 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0058 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0013 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0034 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.8574e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0010 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.1817e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.1677e-04 - val_loss: 0.0057 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.4656e-04 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.8201e-04 - val_loss: 9.7319e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 8.8446e-04 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6.7619e-04 - val_loss: 0.0020 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.0800e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.4713e-04 - val_loss: 9.5996e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 9.9337e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.9581e-04 - val_loss: 0.0012 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.4085e-04 - val_loss: 8.4413e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.3882e-04 - val_loss: 9.8064e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.3617e-04 - val_loss: 8.6319e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.2130e-04 - val_loss: 0.0012 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.9084e-04 - val_loss: 7.9418e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 5.9784e-04 - val_loss: 8.0561e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.7209e-04 - val_loss: 0.0012 - learning_rate: 1.2500e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 5.7330e-04 - val_loss: 0.0013 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.2950e-04 - val_loss: 7.7814e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.6703e-04 - val_loss: 7.7800e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.6125e-04 - val_loss: 7.8541e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 42/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.9221e-04 - val_loss: 9.8233e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.9924e-04 - val_loss: 9.1763e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.6579e-04 - val_loss: 7.9572e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.4738e-04 - val_loss: 8.8343e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.9578e-04 - val_loss: 7.6081e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 6.1895e-04 - val_loss: 7.6335e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6.4974e-04 - val_loss: 8.2796e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.6935e-04 - val_loss: 7.5138e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 7.9771e-04 - val_loss: 7.4905e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 6.8576e-04 - val_loss: 7.4962e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 5.7961e-04 - val_loss: 7.5260e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 6.2106e-04 - val_loss: 7.5830e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.3410e-04 - val_loss: 7.6340e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.7676e-04 - val_loss: 7.5076e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 56/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 5.7187e-04 - val_loss: 7.5500e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 57/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.7656e-04 - val_loss: 7.4593e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 58/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 5.8260e-04 - val_loss: 7.4351e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 59/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 5.3550e-04 - val_loss: 7.5212e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 60/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 5.2804e-04 - val_loss: 7.4217e-04 - learning_rate: 7.8125e-06\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 0.009609\n",
            "  MSE: 0.000270\n",
            "  R²: 0.974924\n",
            "\n",
            "[INFO] Average Cross-Validation Metrics for Dogecoin:\n",
            "  MAE = 0.019955\n",
            "  MSE = 0.005119\n",
            "  R²  = -1.337156\n",
            "\n",
            "[INFO] ----------- FINAL RETRAIN on ALL data -----------\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training final model on the entire dataset (X, y) ...\n",
            "Epoch 1/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.3153 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m17/83\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0565"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0411 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 9.4255e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.0588e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.7627e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.7097e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.2752e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.0537e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.4768e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.6280e-04 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.9950e-04 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.2386e-04 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.4194e-04 - learning_rate: 0.0010\n",
            "Epoch 32/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.2341e-04 - learning_rate: 0.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 34/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.6068e-04 - learning_rate: 0.0010\n",
            "Epoch 35/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.1219e-04 - learning_rate: 0.0010\n",
            "Epoch 36/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.2951e-04 - learning_rate: 0.0010\n",
            "Epoch 37/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 7.2963e-04 - learning_rate: 0.0010\n",
            "Epoch 38/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 7.8700e-04 - learning_rate: 0.0010\n",
            "Epoch 39/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 40/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.5974e-04 - learning_rate: 0.0010\n",
            "Epoch 41/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.4553e-04 - learning_rate: 0.0010\n",
            "Epoch 42/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 43/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 7.1688e-04 - learning_rate: 0.0010\n",
            "Epoch 44/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 8.7935e-04 - learning_rate: 0.0010\n",
            "Epoch 45/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 46/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.7589e-04 - learning_rate: 0.0010\n",
            "Epoch 47/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.2073e-04 - learning_rate: 0.0010\n",
            "Epoch 48/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 8.9820e-04 - learning_rate: 0.0010\n",
            "Epoch 49/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.6465e-04 - learning_rate: 0.0010\n",
            "Epoch 50/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.4677e-04 - learning_rate: 0.0010\n",
            "Epoch 51/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.2109e-04 - learning_rate: 0.0010\n",
            "Epoch 52/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.2395e-04 - learning_rate: 0.0010\n",
            "Epoch 53/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 54/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.5831e-04 - learning_rate: 0.0010\n",
            "Epoch 55/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.8024e-04 - learning_rate: 0.0010\n",
            "Epoch 56/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.8609e-04 - learning_rate: 0.0010\n",
            "Epoch 57/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 7.6919e-04 - learning_rate: 0.0010\n",
            "Epoch 58/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 7.9356e-04 - learning_rate: 0.0010\n",
            "Epoch 59/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.3498e-04 - learning_rate: 0.0010\n",
            "Epoch 60/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.5267e-04 - learning_rate: 0.0010\n",
            "[INFO] Predicting next 5 days based on last window of scaled data...\n",
            "[INFO] Predicting the next 5 days...\n",
            "[INFO] Predictions for 5 future days generated.\n",
            "[INFO] 5-day future predictions for Dogecoin complete.\n",
            "[INFO] Creating a 4-subplot chart for last 7 days + next 5 days ...\n",
            "[INFO] Plot saved as: doge_prediction.png\n",
            "\n",
            "========================================\n",
            "[INFO] PROCESSING ETHEREUM\n",
            "========================================\n",
            "[INFO] Selecting required features: ['Price', 'High', 'Low', 'Close']\n",
            "[INFO] After dropping NaNs, data shape for Ethereum: (2663, 4)\n",
            "[INFO] Applying MinMaxScaler on 4 features.\n",
            "[INFO] Scaling complete. Example of scaled values:\n",
            "               Price      High       Low     Close\n",
            "Date                                              \n",
            "2017-11-09  0.050040  0.048375  0.047477  0.050789\n",
            "2017-11-10  0.045464  0.045675  0.050022  0.049804\n",
            "2017-11-11  0.048727  0.046462  0.045348  0.048708\n",
            "2017-11-12  0.047295  0.046531  0.048756  0.048646\n",
            "2017-11-13  0.049158  0.048368  0.047134  0.050573\n",
            "[INFO] Creating sliding windows with window_len=10 ...\n",
            "[INFO] Created X shape: (2653, 10, 4) (samples, window_len, features)\n",
            "[INFO] y shape: (2653, 4) (samples, features)\n",
            "[INFO] Performing 5-fold TimeSeriesSplit cross-validation on Ethereum...\n",
            "\n",
            "[INFO] ---- Fold 1 of 5 ----\n",
            "[INFO] X_train shape: (443, 10, 4), y_train shape: (443, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - loss: 0.4187 - val_loss: 0.3250 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2995 - val_loss: 0.2296 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2112 - val_loss: 0.1599 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1469 - val_loss: 0.1102 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1010 - val_loss: 0.0751 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0688 - val_loss: 0.0509 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0466 - val_loss: 0.0341 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0314 - val_loss: 0.0229 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0211 - val_loss: 0.0153 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0141 - val_loss: 0.0101 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0094 - val_loss: 0.0068 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0065 - val_loss: 0.0045 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0032 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 7.9550e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.5198e-04 - val_loss: 5.8420e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.9738e-04 - val_loss: 4.2256e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.5322e-04 - val_loss: 3.2683e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 4.6032e-04 - val_loss: 2.5944e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 4.4785e-04 - val_loss: 2.7116e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 3.8031e-04 - val_loss: 2.3687e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 4.6246e-04 - val_loss: 1.9237e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.3378e-04 - val_loss: 1.4690e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.7963e-04 - val_loss: 1.3844e-04 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.8855e-04 - val_loss: 1.3027e-04 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.9093e-04 - val_loss: 2.3869e-04 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.5111e-04 - val_loss: 1.3441e-04 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 2.8013e-04 - val_loss: 1.1228e-04 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.6053e-04 - val_loss: 2.8770e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.7540e-04 - val_loss: 1.8889e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 2.5139e-04 - val_loss: 1.0384e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.7552e-04 - val_loss: 1.2208e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.4672e-04 - val_loss: 1.0462e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 2.2153e-04 - val_loss: 9.6607e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.3249e-04 - val_loss: 9.8091e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.2541e-04 - val_loss: 9.8138e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.0628e-04 - val_loss: 1.1641e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.1486e-04 - val_loss: 9.5320e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.9080e-04 - val_loss: 9.4574e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.1381e-04 - val_loss: 9.4078e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2.0419e-04 - val_loss: 9.4656e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.9230e-04 - val_loss: 9.2417e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.9430e-04 - val_loss: 8.7435e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.9330e-04 - val_loss: 8.6805e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.1007e-04 - val_loss: 8.6091e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8769e-04 - val_loss: 8.6067e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.8475e-04 - val_loss: 8.5830e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2.1894e-04 - val_loss: 8.8275e-05 - learning_rate: 6.2500e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2.0877e-04 - val_loss: 8.6211e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.7630e-04 - val_loss: 8.4454e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.0991e-04 - val_loss: 8.4360e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.8483e-04 - val_loss: 9.0152e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.1286e-04 - val_loss: 8.3831e-05 - learning_rate: 3.1250e-05\n",
            "Epoch 56/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.8512e-04 - val_loss: 8.3945e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 57/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 2.0332e-04 - val_loss: 8.4316e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 58/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.0351e-04 - val_loss: 8.4516e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 59/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.0588e-04 - val_loss: 8.3746e-05 - learning_rate: 1.5625e-05\n",
            "Epoch 60/60\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 2.0153e-04 - val_loss: 8.3529e-05 - learning_rate: 1.5625e-05\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 9.774578\n",
            "  MSE: 180.374333\n",
            "  R²: 0.922568\n",
            "\n",
            "[INFO] ---- Fold 2 of 5 ----\n",
            "[INFO] X_train shape: (885, 10, 4), y_train shape: (885, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.3822 - val_loss: 0.2328 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1909 - val_loss: 0.1106 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0903 - val_loss: 0.0523 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0412 - val_loss: 0.0253 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0183 - val_loss: 0.0132 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.0160 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0119 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0016 - val_loss: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 9.1847e-04 - val_loss: 0.0170 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7802e-04 - val_loss: 0.0149 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7431e-04 - val_loss: 0.0169 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.0897e-04 - val_loss: 0.0167 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.3558e-04 - val_loss: 0.0191 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1960e-04 - val_loss: 0.0152 - learning_rate: 5.0000e-04\n",
            "Epoch 15/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.9293e-04 - val_loss: 0.0178 - learning_rate: 5.0000e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1.9521e-04 - val_loss: 0.0140 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.7486e-04 - val_loss: 0.0161 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1.6863e-04 - val_loss: 0.0185 - learning_rate: 5.0000e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 236.071942\n",
            "  MSE: 208523.053339\n",
            "  R²: 0.774657\n",
            "\n",
            "[INFO] ---- Fold 3 of 5 ----\n",
            "[INFO] X_train shape: (1327, 10, 4), y_train shape: (1327, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.3759 - val_loss: 0.1738 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.1346 - val_loss: 0.0635 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0476 - val_loss: 0.0259 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0180 - val_loss: 0.0159 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0080 - val_loss: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0043 - val_loss: 0.0079 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0076 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0071 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0057 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0125 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0050 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 0.0060 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0015 - val_loss: 0.0096 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0046 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0133 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 8.7357e-04 - val_loss: 0.0060 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.2500e-04 - val_loss: 0.0045 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0057 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 8.9960e-04 - val_loss: 0.0054 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.3490e-04 - val_loss: 0.0083 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0046 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.8199e-04 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.3752e-04 - val_loss: 0.0057 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.6238e-04 - val_loss: 0.0045 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 9.7785e-04 - val_loss: 0.0048 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 8.3670e-04 - val_loss: 0.0033 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.8206e-04 - val_loss: 0.0053 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.3616e-04 - val_loss: 0.0034 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.5912e-04 - val_loss: 0.0100 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 8.7472e-04 - val_loss: 0.0048 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 7.7287e-04 - val_loss: 0.0049 - learning_rate: 5.0000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 7.6096e-04 - val_loss: 0.0042 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.4666e-04 - val_loss: 0.0053 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.8599e-04 - val_loss: 0.0037 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.4062e-04 - val_loss: 0.0029 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.6502e-04 - val_loss: 0.0032 - learning_rate: 2.5000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.0411e-04 - val_loss: 0.0031 - learning_rate: 2.5000e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.2063e-04 - val_loss: 0.0042 - learning_rate: 2.5000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.5252e-04 - val_loss: 0.0034 - learning_rate: 2.5000e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.6478e-04 - val_loss: 0.0037 - learning_rate: 2.5000e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.9477e-04 - val_loss: 0.0045 - learning_rate: 1.2500e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.7707e-04 - val_loss: 0.0031 - learning_rate: 1.2500e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.1845e-04 - val_loss: 0.0029 - learning_rate: 1.2500e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.9161e-04 - val_loss: 0.0040 - learning_rate: 1.2500e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.5384e-04 - val_loss: 0.0040 - learning_rate: 1.2500e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 185.056534\n",
            "  MSE: 59758.878424\n",
            "  R²: 0.937999\n",
            "\n",
            "[INFO] ---- Fold 4 of 5 ----\n",
            "[INFO] X_train shape: (1769, 10, 4), y_train shape: (1769, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.3845 - val_loss: 0.1440 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.1135 - val_loss: 0.0490 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0413 - val_loss: 0.0209 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0192 - val_loss: 0.0117 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0124 - val_loss: 0.0084 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0085 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0068 - val_loss: 0.0043 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0053 - val_loss: 0.0047 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0039 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0025 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 9.6736e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 8.6648e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 8.0373e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 8.2498e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 7.9573e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 7.1417e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 7.5652e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 6.3686e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 7.6786e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 6.8001e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 6.2123e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 6.3772e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 5.8071e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 5.7824e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 7.2494e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 7.1196e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 5.4730e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 5.3921e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 5.4595e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 5.5971e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 9.9534e-04 - val_loss: 5.4677e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.9273e-04 - val_loss: 7.0869e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 5.3681e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 9.6036e-04 - val_loss: 6.5971e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 9.2065e-04 - val_loss: 5.5100e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.1664e-04 - val_loss: 5.0904e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.6757e-04 - val_loss: 5.3104e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.3631e-04 - val_loss: 5.2498e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.5723e-04 - val_loss: 5.4281e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.7181e-04 - val_loss: 5.2976e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.8241e-04 - val_loss: 6.0049e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.7873e-04 - val_loss: 5.2248e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 9.3091e-04 - val_loss: 5.4438e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.1371e-04 - val_loss: 5.2667e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 56/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 9.1504e-04 - val_loss: 5.1658e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 57/60\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 9.1909e-04 - val_loss: 5.3951e-04 - learning_rate: 1.5625e-05\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 43.861697\n",
            "  MSE: 3395.959664\n",
            "  R²: 0.949377\n",
            "\n",
            "[INFO] ---- Fold 5 of 5 ----\n",
            "[INFO] X_train shape: (2211, 10, 4), y_train shape: (2211, 4)\n",
            "[INFO] X_test  shape: (442, 10, 4),  y_test  shape: (442, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 0.3504 - val_loss: 0.1053 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0763 - val_loss: 0.0291 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0233 - val_loss: 0.0129 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0113 - val_loss: 0.0079 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0075 - val_loss: 0.0064 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0043 - val_loss: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0036 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0035 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0019 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0039 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 0.0022 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0035 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 0.0017 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0030 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0014 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0017 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 0.0018 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 9.2605e-04 - val_loss: 0.0014 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 2.5000e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 102.332472\n",
            "  MSE: 19061.384495\n",
            "  R²: 0.925493\n",
            "\n",
            "[INFO] Average Cross-Validation Metrics for Ethereum:\n",
            "  MAE = 115.419445\n",
            "  MSE = 58183.930051\n",
            "  R²  = 0.902019\n",
            "\n",
            "[INFO] ----------- FINAL RETRAIN on ALL data -----------\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training final model on the entire dataset (X, y) ...\n",
            "Epoch 1/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.3581 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m 1/83\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 167ms/step - loss: 0.0920"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0685 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0234 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0081 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0063 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0045 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0036 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0029 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 32/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 34/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 35/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 36/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 37/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.6015e-04 - learning_rate: 0.0010\n",
            "Epoch 38/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 39/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 40/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 41/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 42/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.6424e-04 - learning_rate: 0.0010\n",
            "Epoch 43/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 44/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.8109e-04 - learning_rate: 0.0010\n",
            "Epoch 45/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 46/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 47/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.8015e-04 - learning_rate: 0.0010\n",
            "Epoch 48/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.1579e-04 - learning_rate: 0.0010\n",
            "Epoch 49/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 50/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 51/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 52/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.3379e-04 - learning_rate: 0.0010\n",
            "Epoch 53/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 54/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.4670e-04 - learning_rate: 0.0010\n",
            "Epoch 55/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.9533e-04 - learning_rate: 0.0010\n",
            "Epoch 56/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 57/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 9.4221e-04 - learning_rate: 0.0010\n",
            "Epoch 58/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 59/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.7325e-04 - learning_rate: 0.0010\n",
            "Epoch 60/60\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "[INFO] Predicting next 5 days based on last window of scaled data...\n",
            "[INFO] Predicting the next 5 days...\n",
            "[INFO] Predictions for 5 future days generated.\n",
            "[INFO] 5-day future predictions for Ethereum complete.\n",
            "[INFO] Creating a 4-subplot chart for last 7 days + next 5 days ...\n",
            "[INFO] Plot saved as: eth_prediction.png\n",
            "\n",
            "========================================\n",
            "[INFO] PROCESSING LITECOIN\n",
            "========================================\n",
            "[INFO] Selecting required features: ['Price', 'High', 'Low', 'Close']\n",
            "[INFO] After dropping NaNs, data shape for Litecoin: (3812, 4)\n",
            "[INFO] Applying MinMaxScaler on 4 features.\n",
            "[INFO] Scaling complete. Example of scaled values:\n",
            "               Price      High       Low     Close\n",
            "Date                                              \n",
            "2014-09-17  0.010126  0.011192  0.010169  0.009295\n",
            "2014-09-18  0.009157  0.010071  0.010116  0.009039\n",
            "2014-09-19  0.008229  0.009125  0.009139  0.008287\n",
            "2014-09-20  0.008122  0.008973  0.008213  0.007947\n",
            "2014-09-21  0.008017  0.008836  0.008042  0.007180\n",
            "[INFO] Creating sliding windows with window_len=10 ...\n",
            "[INFO] Created X shape: (3802, 10, 4) (samples, window_len, features)\n",
            "[INFO] y shape: (3802, 4) (samples, features)\n",
            "[INFO] Performing 5-fold TimeSeriesSplit cross-validation on Litecoin...\n",
            "\n",
            "[INFO] ---- Fold 1 of 5 ----\n",
            "[INFO] X_train shape: (637, 10, 4), y_train shape: (637, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.3973 - val_loss: 0.3317 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.2448 - val_loss: 0.2212 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.1455 - val_loss: 0.1530 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0843 - val_loss: 0.1123 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0479 - val_loss: 0.0887 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0267 - val_loss: 0.0744 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0146 - val_loss: 0.0669 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0079 - val_loss: 0.0629 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0605 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0592 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0585 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.5878e-04 - val_loss: 0.0585 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.5458e-04 - val_loss: 0.0580 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.9042e-04 - val_loss: 0.0579 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.0329e-04 - val_loss: 0.0579 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.6487e-05 - val_loss: 0.0579 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.1784e-05 - val_loss: 0.0580 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.9980e-05 - val_loss: 0.0577 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.3553e-05 - val_loss: 0.0578 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 9.4859e-06 - val_loss: 0.0579 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.3034e-06 - val_loss: 0.0579 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.5256e-06 - val_loss: 0.0580 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.9563e-06 - val_loss: 0.0578 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1184e-06 - val_loss: 0.0580 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.4564e-06 - val_loss: 0.0580 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 6.8855e-06 - val_loss: 0.0580 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 7.0894e-06 - val_loss: 0.0578 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 7.3252e-06 - val_loss: 0.0580 - learning_rate: 5.0000e-04\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 50.732472\n",
            "  MSE: 8413.923704\n",
            "  R²: -0.442239\n",
            "\n",
            "[INFO] ---- Fold 2 of 5 ----\n",
            "[INFO] X_train shape: (1270, 10, 4), y_train shape: (1270, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.3787 - val_loss: 0.1801 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1454 - val_loss: 0.0717 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0555 - val_loss: 0.0278 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0226 - val_loss: 0.0114 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0101 - val_loss: 0.0086 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0064 - val_loss: 0.0038 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0027 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0095 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 9.7542e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0047 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 8.7281e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 7.7885e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0043 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 6.9237e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 9.5155e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 7.3906e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 6.0437e-04 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 6.9701e-04 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 6.6853e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 7.3134e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 8.4142e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.9772e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 9.9787e-04 - val_loss: 7.5953e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 9.6503e-04 - val_loss: 9.2257e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.3857e-04 - val_loss: 5.5767e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 9.2193e-04 - val_loss: 6.3416e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 6.1021e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 8.2297e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.6122e-04 - val_loss: 6.3446e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 8.8153e-04 - val_loss: 6.5072e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 9.4298e-04 - val_loss: 6.5412e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.8381e-04 - val_loss: 9.3900e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 8.5396e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.8781e-04 - val_loss: 5.3112e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.1424e-04 - val_loss: 7.4203e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 8.7632e-04 - val_loss: 5.4364e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 8.1730e-04 - val_loss: 5.8370e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 4.9489e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.5842e-04 - val_loss: 6.3663e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 7.5561e-04 - val_loss: 6.4479e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 7.6722e-04 - val_loss: 5.9676e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.3989e-04 - val_loss: 5.1681e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 7.0292e-04 - val_loss: 5.3417e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.6600e-04 - val_loss: 5.6577e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 8.5790e-04 - val_loss: 5.3070e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 7.0552e-04 - val_loss: 5.4195e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 56/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 8.3055e-04 - val_loss: 5.6430e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 57/60\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 9.0386e-04 - val_loss: 5.4737e-04 - learning_rate: 1.5625e-05\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 4.335896\n",
            "  MSE: 34.127900\n",
            "  R²: 0.971044\n",
            "\n",
            "[INFO] ---- Fold 3 of 5 ----\n",
            "[INFO] X_train shape: (1903, 10, 4), y_train shape: (1903, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 0.3476 - val_loss: 0.1142 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0828 - val_loss: 0.0284 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0207 - val_loss: 0.0169 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0072 - val_loss: 0.0053 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0048 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0047 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0026 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0039 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0038 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0029 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0029 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0055 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0019 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0028 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0017 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 9.9416e-04 - val_loss: 0.0014 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.6536e-04 - val_loss: 0.0030 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.3869e-04 - val_loss: 0.0025 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.8043e-04 - val_loss: 0.0017 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.8949e-04 - val_loss: 0.0017 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.3118e-04 - val_loss: 0.0014 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.1923e-04 - val_loss: 0.0013 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.2491e-04 - val_loss: 0.0014 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.7615e-04 - val_loss: 0.0015 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.5748e-04 - val_loss: 0.0014 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.1358e-04 - val_loss: 0.0014 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 6.8408e-04 - val_loss: 0.0013 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.9321e-04 - val_loss: 0.0012 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5.8331e-04 - val_loss: 0.0012 - learning_rate: 1.2500e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.5018e-04 - val_loss: 0.0012 - learning_rate: 1.2500e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.7425e-04 - val_loss: 0.0012 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.8984e-04 - val_loss: 0.0013 - learning_rate: 1.2500e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.5894e-04 - val_loss: 0.0012 - learning_rate: 6.2500e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.0805e-04 - val_loss: 0.0012 - learning_rate: 6.2500e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.0703e-04 - val_loss: 0.0013 - learning_rate: 6.2500e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.8743e-04 - val_loss: 0.0014 - learning_rate: 6.2500e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.5614e-04 - val_loss: 0.0013 - learning_rate: 6.2500e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.1062e-04 - val_loss: 0.0012 - learning_rate: 3.1250e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 6.0567e-04 - val_loss: 0.0012 - learning_rate: 3.1250e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.4774e-04 - val_loss: 0.0012 - learning_rate: 3.1250e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.4193e-04 - val_loss: 0.0012 - learning_rate: 3.1250e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.0272e-04 - val_loss: 0.0013 - learning_rate: 3.1250e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.8713e-04 - val_loss: 0.0012 - learning_rate: 1.5625e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.6852e-04 - val_loss: 0.0012 - learning_rate: 1.5625e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5.9632e-04 - val_loss: 0.0012 - learning_rate: 1.5625e-05\n",
            "Epoch 56/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.6011e-04 - val_loss: 0.0012 - learning_rate: 1.5625e-05\n",
            "Epoch 57/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.8704e-04 - val_loss: 0.0012 - learning_rate: 1.5625e-05\n",
            "Epoch 58/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.6998e-04 - val_loss: 0.0013 - learning_rate: 1.5625e-05\n",
            "Epoch 59/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7.2620e-04 - val_loss: 0.0012 - learning_rate: 7.8125e-06\n",
            "Epoch 60/60\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 5.6684e-04 - val_loss: 0.0012 - learning_rate: 7.8125e-06\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 6.204008\n",
            "  MSE: 139.707500\n",
            "  R²: 0.974000\n",
            "\n",
            "[INFO] ---- Fold 4 of 5 ----\n",
            "[INFO] X_train shape: (2536, 10, 4), y_train shape: (2536, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.3179 - val_loss: 0.0716 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0496 - val_loss: 0.0134 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0113 - val_loss: 0.0050 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0053 - val_loss: 0.0040 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0034 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 8.2790e-04 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0033 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 8.2677e-04 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 8.5791e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 6.9461e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 7.7837e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 7.1576e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 6.9449e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 6.4000e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 6.7089e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 7.9960e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 7.8990e-04 - val_loss: 7.6620e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 7.8645e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.0297e-04 - val_loss: 6.3131e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.0992e-04 - val_loss: 5.7380e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.4022e-04 - val_loss: 5.2608e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.0424e-04 - val_loss: 8.1640e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 5.3452e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 5.9330e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.7458e-04 - val_loss: 5.0179e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.2817e-04 - val_loss: 5.0682e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6.5474e-04 - val_loss: 5.3057e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 7.9480e-04 - val_loss: 4.9053e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.6649e-04 - val_loss: 5.1215e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.7094e-04 - val_loss: 4.8156e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.0031e-04 - val_loss: 4.7243e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 40/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.5691e-04 - val_loss: 4.9466e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 41/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.1475e-04 - val_loss: 4.7045e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 42/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.9366e-04 - val_loss: 4.7947e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.7811e-04 - val_loss: 5.8647e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 7.9476e-04 - val_loss: 4.7281e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 6.2205e-04 - val_loss: 4.8448e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 7.8491e-04 - val_loss: 4.6577e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.8459e-04 - val_loss: 4.6015e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.5826e-04 - val_loss: 4.5939e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.7522e-04 - val_loss: 4.6431e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.0554e-04 - val_loss: 4.5555e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.7385e-04 - val_loss: 4.5444e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.1120e-04 - val_loss: 4.7743e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.4099e-04 - val_loss: 4.6269e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6.6600e-04 - val_loss: 4.5869e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 5.7067e-04 - val_loss: 4.8231e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 56/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.2012e-04 - val_loss: 4.7445e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 57/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.0509e-04 - val_loss: 4.6291e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 58/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.8305e-04 - val_loss: 4.5455e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 59/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.4435e-04 - val_loss: 4.5835e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 60/60\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.0497e-04 - val_loss: 4.6030e-04 - learning_rate: 3.9063e-06\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 3.780131\n",
            "  MSE: 36.919316\n",
            "  R²: 0.983387\n",
            "\n",
            "[INFO] ---- Fold 5 of 5 ----\n",
            "[INFO] X_train shape: (3169, 10, 4), y_train shape: (3169, 4)\n",
            "[INFO] X_test  shape: (633, 10, 4),  y_test  shape: (633, 4)\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training model...\n",
            "Epoch 1/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.2987 - val_loss: 0.0473 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0320 - val_loss: 0.0081 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0076 - val_loss: 0.0048 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0046 - val_loss: 0.0019 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 8.8445e-04 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 7.5527e-04 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 6.6577e-04 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 5.3979e-04 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 6.6758e-04 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 6.0024e-04 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 9.8636e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 5.7417e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 7.2405e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 4.9034e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 9.4003e-04 - val_loss: 5.3298e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.0506e-04 - val_loss: 5.2405e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 4.6749e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 9.1285e-04 - val_loss: 4.9443e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 9.0341e-04 - val_loss: 3.9147e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.9633e-04 - val_loss: 3.7647e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.1208e-04 - val_loss: 3.7968e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.0937e-04 - val_loss: 4.8444e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 8.7692e-04 - val_loss: 5.2594e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.9505e-04 - val_loss: 3.6874e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.6180e-04 - val_loss: 3.4219e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.4297e-04 - val_loss: 3.4389e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 3.6952e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 6.8286e-04 - val_loss: 4.2955e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 8.4900e-04 - val_loss: 3.4434e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 7.3239e-04 - val_loss: 3.3906e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 34/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.5968e-04 - val_loss: 3.5365e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 35/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.1316e-04 - val_loss: 3.3137e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 36/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.6618e-04 - val_loss: 3.3661e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 37/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.8054e-04 - val_loss: 3.2478e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 38/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.5755e-04 - val_loss: 3.2433e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 39/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 7.2318e-04 - val_loss: 3.2279e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 40/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.0644e-04 - val_loss: 3.2036e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 41/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.1850e-04 - val_loss: 3.2901e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 42/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.3053e-04 - val_loss: 3.2420e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.3820e-04 - val_loss: 3.2109e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 5.9854e-04 - val_loss: 3.2487e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.4211e-04 - val_loss: 3.1739e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 6.7707e-04 - val_loss: 3.1707e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 6.7455e-04 - val_loss: 3.1788e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 6.9557e-04 - val_loss: 3.1724e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 49/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.1590e-04 - val_loss: 3.2082e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 50/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.8222e-04 - val_loss: 3.1631e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 51/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.1722e-04 - val_loss: 3.1727e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 52/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 6.4268e-04 - val_loss: 3.1679e-04 - learning_rate: 7.8125e-06\n",
            "Epoch 53/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 6.6779e-04 - val_loss: 3.1742e-04 - learning_rate: 3.9063e-06\n",
            "Epoch 54/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.2378e-04 - val_loss: 3.1743e-04 - learning_rate: 3.9063e-06\n",
            "Epoch 55/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.2832e-04 - val_loss: 3.1581e-04 - learning_rate: 3.9063e-06\n",
            "Epoch 56/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 5.5708e-04 - val_loss: 3.1643e-04 - learning_rate: 3.9063e-06\n",
            "Epoch 57/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 6.4995e-04 - val_loss: 3.1596e-04 - learning_rate: 3.9063e-06\n",
            "Epoch 58/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.6048e-04 - val_loss: 3.1593e-04 - learning_rate: 1.9531e-06\n",
            "Epoch 59/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 6.4370e-04 - val_loss: 3.1706e-04 - learning_rate: 1.9531e-06\n",
            "Epoch 60/60\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 6.3737e-04 - val_loss: 3.1682e-04 - learning_rate: 1.9531e-06\n",
            "[INFO] Evaluating fold performance...\n",
            "[INFO] Fold metrics:\n",
            "  MAE: 2.772226\n",
            "  MSE: 18.166890\n",
            "  R²: 0.941373\n",
            "\n",
            "[INFO] Average Cross-Validation Metrics for Litecoin:\n",
            "  MAE = 13.564947\n",
            "  MSE = 1728.569062\n",
            "  R²  = 0.685513\n",
            "\n",
            "[INFO] ----------- FINAL RETRAIN on ALL data -----------\n",
            "[INFO] Building a stacked Bidirectional LSTM + Attention model...\n",
            "[INFO] Model built and compiled.\n",
            "[INFO] Training final model on the entire dataset (X, y) ...\n",
            "Epoch 1/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.2783 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m 16/119\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0276"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0190 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0045 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.1734e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.8651e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.8496e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.1974e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 9.0504e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.0835e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.4038e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.2770e-04 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.1198e-04 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 7.8907e-04 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 7.4209e-04 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.9364e-04 - learning_rate: 0.0010\n",
            "Epoch 32/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.2469e-04 - learning_rate: 0.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 34/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.6862e-04 - learning_rate: 0.0010\n",
            "Epoch 35/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 36/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.7186e-04 - learning_rate: 0.0010\n",
            "Epoch 37/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6.8946e-04 - learning_rate: 0.0010\n",
            "Epoch 38/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 8.2392e-04 - learning_rate: 0.0010\n",
            "Epoch 39/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 40/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.9657e-04 - learning_rate: 0.0010\n",
            "Epoch 41/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.7467e-04 - learning_rate: 0.0010\n",
            "Epoch 42/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 8.6318e-04 - learning_rate: 0.0010\n",
            "Epoch 43/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 7.4515e-04 - learning_rate: 0.0010\n",
            "Epoch 44/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.6449e-04 - learning_rate: 0.0010\n",
            "Epoch 45/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.1061e-04 - learning_rate: 0.0010\n",
            "Epoch 46/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 6.6741e-04 - learning_rate: 0.0010\n",
            "Epoch 47/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 6.3401e-04 - learning_rate: 0.0010\n",
            "Epoch 48/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 9.7879e-04 - learning_rate: 0.0010\n",
            "Epoch 49/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 7.4559e-04 - learning_rate: 0.0010\n",
            "Epoch 50/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.4667e-04 - learning_rate: 0.0010\n",
            "Epoch 51/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.8671e-04 - learning_rate: 0.0010\n",
            "Epoch 52/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.3911e-04 - learning_rate: 0.0010\n",
            "Epoch 53/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 6.6700e-04 - learning_rate: 0.0010\n",
            "Epoch 54/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 8.4354e-04 - learning_rate: 0.0010\n",
            "Epoch 55/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.1226e-04 - learning_rate: 0.0010\n",
            "Epoch 56/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.0356e-04 - learning_rate: 0.0010\n",
            "Epoch 57/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.2204e-04 - learning_rate: 0.0010\n",
            "Epoch 58/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.3874e-04 - learning_rate: 0.0010\n",
            "Epoch 59/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 7.4507e-04 - learning_rate: 0.0010\n",
            "Epoch 60/60\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.7700e-04 - learning_rate: 0.0010\n",
            "[INFO] Predicting next 5 days based on last window of scaled data...\n",
            "[INFO] Predicting the next 5 days...\n",
            "[INFO] Predictions for 5 future days generated.\n",
            "[INFO] 5-day future predictions for Litecoin complete.\n",
            "[INFO] Creating a 4-subplot chart for last 7 days + next 5 days ...\n",
            "[INFO] Plot saved as: ltc_prediction.png\n",
            "\n",
            "[INFO] Saving all predictions to btc_doge_eth_ltc_predictions.csv ...\n",
            "[INFO] Predictions saved successfully. Preview:\n",
            "             Price             High              Low            Close crypto_name       Date\n",
            "0 93819.4687500000 92845.3750000000 95606.6093750000 97824.8046875000     Bitcoin 2025-02-23\n",
            "1 93576.2578125000 92598.9218750000 95349.2265625000 97559.5625000000     Bitcoin 2025-02-24\n",
            "2 93366.0781250000 92386.0078125000 95126.8906250000 97330.4609375000     Bitcoin 2025-02-25\n",
            "3 93185.4218750000 92203.0312500000 94935.8906250000 97133.6328125000     Bitcoin 2025-02-26\n",
            "4 93032.6875000000 92048.3828125000 94774.4687500000 96967.2968750000     Bitcoin 2025-02-27\n",
            "5     0.2297251672     0.2151895314     0.2238200009     0.2402429432    Dogecoin 2025-02-23\n",
            "6     0.2237233818     0.2096971571     0.2180302441     0.2338864803    Dogecoin 2025-02-24\n",
            "7     0.2178223431     0.2042929828     0.2123360634     0.2276379019    Dogecoin 2025-02-25\n",
            "8     0.2120773047     0.1990279406     0.2067908794     0.2215556800    Dogecoin 2025-02-26\n",
            "9     0.2065098137     0.1939220577     0.2014156729     0.2156624794    Dogecoin 2025-02-27\n"
          ]
        }
      ]
    }
  ]
}